<!DOCTYPE html>
<html>
<head>
    <title>Probability and Statistics:<br/>To p or not to p?</title>
</head>
<body class='scrollbar'>
    
    <div style='display: flex; flex-direction: row; margin: 2vh 2vw 5vh; justify-content: center;'>
        <div id='sideleft' style='flex: 1; max-width: 20vw'>
            <div id='sidebar' class='scrollbar'>
                    
    <ul>
    
        <div style='white-space: nowrap; text-overflow: ellipsis; overflow: hidden !important;'>
            <a href='/probability-statistics/#dealing-with-uncertainty-and-complexity-in-a-chaotic-world' data-tooltip='Dealing with Uncertainty and Complexity in a Chaotic World'>
                
                    Dealing with Uncertainty and Complexity in a Chaotic World
                
            </a>
        </div>
        
    
        <div style='white-space: nowrap; text-overflow: ellipsis; overflow: hidden !important;'>
            <a href='/probability-statistics/#quantifying-uncertainty-with-probability' data-tooltip='Quantifying Uncertainty With Probability'>
                
                    Quantifying Uncertainty With Probability
                
            </a>
        </div>
        
    
        <div style='white-space: nowrap; text-overflow: ellipsis; overflow: hidden !important;'>
            <a href='/probability-statistics/#describing-the-world-the-statistical-way' data-tooltip='Describing The World The Statistical Way'>
                
                    Describing The World The Statistical Way
                
            </a>
        </div>
        
    
        <div style='white-space: nowrap; text-overflow: ellipsis; overflow: hidden !important;'>
            <a href='/probability-statistics/#on-your-marks-get-set-infer' data-tooltip='On Your Marks, Get Set, Infer!'>
                
                    On Your Marks, Get Set, Infer!
                
            </a>
        </div>
        
    
        <div style='white-space: nowrap; text-overflow: ellipsis; overflow: hidden !important;'>
            <a href='/probability-statistics/#to-p-or-not-to-p' data-tooltip='To p Or Not To p?'>
                
                    To p Or Not To p?
                
            </a>
        </div>
        
    
        <div style='white-space: nowrap; text-overflow: ellipsis; overflow: hidden !important;'>
            <a href='/probability-statistics/#applications' data-tooltip='Applications'>
                
                    Applications
                
            </a>
        </div>
        
    
    </ul>
            </div>
        </div>
        <div style='flex: 3; max-width: 72vw; padding-left: 20px;'>
            <h1 style='text-align: center; text-transform: uppercase;'>Probability and Statistics:<br/>To p or not to p?</h1>
            <link rel="stylesheet" href="../../assets/style.css" />

<h3 id="dealing-with-uncertainty-and-complexity-in-a-chaotic-world">Dealing with Uncertainty and Complexity in a Chaotic World</h3>

<p><b>
A black swan event is a low-probability high-impact event.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
A black swan event is a high-probability low-impact event.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
A model is a perfect representation of reality.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
A good model, other things equal, departs significantly from reality.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
A good model, other things equal, simplifies reality in order to make a problem easier to understand.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
In the Monty Hall problem, switching to the unopened door guarantees you will win the sports car.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
In the Monty Hall problem, initially all three doors have the same probability of hiding the sports car.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
Internal variables are under our control.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
Under the assumption of normality, there is approximately a 99.7% probability of being within two standard deviations of the mean.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
There is a current skills deficit due to demand from employers for quantitative staff exceeding supply.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
There is a current skills surplus due to the supply of quantitative employees exceeding demand from employers.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
The classic London Underground map is ideal for engineers who maintain the tunnel network.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
The classic London Underground map allows a tourist to get from point A to point B.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
The “Circle line” is a sensible choice of name for the “Circle line”.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
The Circle line is a true (perfect) circle.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
Decision-making is a process when one is faced with a problem or decision having more than one possible outcome.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<h3 id="quantifying-uncertainty-with-probability">Quantifying Uncertainty With Probability</h3>

<p><b>
An event is:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> any subset <em>A</em> of the sample space<br />
☐ <code class="highlighter-rouge">WRONG</code> a probability<br />
☐ <code class="highlighter-rouge">WRONG</code> a function which assigns probabilities to events<br /></p>

<p><b>
Probability…<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> can exceed 1<br />
☐ <code class="highlighter-rouge">WRONG</code> can be negative<br />
⚡ <code class="highlighter-rouge">CORRECT</code> is always some value in the unit interval [0, 1]<br /></p>

<p><b>
Everyone will agree on the exact probabilities of various geopolitical events.<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> True<br />
⚡ <code class="highlighter-rouge">CORRECT</code> False<br /></p>

<p><b>
When there are <em>N</em> equally likely outcomes in a sample space, where none of them agrees with some event <em>A</em>, then:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> <img src="https://latex.codecogs.com/svg.latex?P(A)=0" /><br />
☐ <code class="highlighter-rouge">WRONG</code> <img src="https://latex.codecogs.com/svg.latex?0&lt;P(A)&lt;1" /><br />
☐ <code class="highlighter-rouge">WRONG</code> <img src="https://latex.codecogs.com/svg.latex?P(A)=1" /><br /></p>

<p><b>
The expectation of a (discrete) random variable <em>X</em>:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> is a probability-weighted average<br />
☐ <code class="highlighter-rouge">WRONG</code> is a non-probability-weighted average<br />
☐ <code class="highlighter-rouge">WRONG</code> must be equal to a possible value of <em>X</em><br /></p>

<p><b>
If we rolled a fair die a very large number of times, then we would expect the average score to be approximately:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> 2.5<br />
⚡ <code class="highlighter-rouge">CORRECT</code> 3.5<br />
☐ <code class="highlighter-rouge">WRONG</code> 4.5<br /></p>

<p><b>
For the family of Bernoulli distributions, the parameter π is such that:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> π = 0<br />
⚡ <code class="highlighter-rouge">CORRECT</code> 0 ≤ π ≤ 1<br />
☐ <code class="highlighter-rouge">WRONG</code> π = 1<br /></p>

<p><b>
For any Bernoulli random variable, <img src="https://latex.codecogs.com/svg.latex?P(X=0)" /> is:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> 0<br />
☐ <code class="highlighter-rouge">WRONG</code> π<br />
⚡ <code class="highlighter-rouge">CORRECT</code> 1 − π<br /></p>

<p><b>
If <em>X ∼ Bin(n, π)</em> then <em>E(X) = nπ</em>.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
The Poisson distribution can approximate the binomial distribution under certain limiting conditions.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<h3 id="describing-the-world-the-statistical-way">Describing The World The Statistical Way</h3>

<p><strong>discrete</strong> data : things you can count<br />
<strong>continuous</strong> data : things you can measure<br />
<strong>measurable variables</strong> are those where there is a generally recognised method of measuring the value of the variable of interest.<br />
<strong>categorical variables</strong> are those where no such method exists (or, often enough, is even possible), but among which:<br />
    + <strong>ordinal</strong> (categorical) variables can be put in some sensible order (dissatisfied/indifferent/satisfied)<br />
    + <strong>nominal</strong> (categorical) variables cannot be put in any sensible order, but are only known by their name (identification).<br />
<strong>ranking scale</strong> are numbers to indicate the relative extent to which the cases possess some characteristic of an ordinal variable.<br /></p>

<p><strong>interval measurable variables</strong> are variables have scales where numerically equal distances on the scale represent equal value differences in the characteristic being measured (temperature)<br />
    + the location of the <strong>zero point</strong> is not fixed – both the zero point and the units of measurement are arbitrary (Celsius vs Fahrenheit).<br />
    + any <strong>positive linear transformation</strong> of the form y = a + bx will preserve the properties of the scale, hence it is not meaningful to take ratios of scale values.<br /></p>

<p><strong>ratio measurable variables</strong><br />
    + possess all the properties of nominal, ordinal and interval variables.<br />
    + has an absolute zero point and it is meaningful to compute ratios of scale values.<br />
    + only <strong>proportionate transformations</strong> of the form y = bx, where b is a positive constant, are allowed.<br /></p>

<p><strong>binary</strong> or <strong>dichotomous</strong> variable is the variable with just two possible values<br />
<strong>descriptive statistics</strong>: summarise the data which were collected, in order to make them more understandable.<br />
<strong>statistical inference</strong>: use the observed data to draw conclusions about some broader population.<br /></p>

<p><strong>data matrix</strong> is used to store the statistical data in a sample.<br />
    + rows of the data matrix correspond to different <strong>units</strong> (subjects/observations).<br />
    + the number of units in a dataset is the <strong>sample size</strong>, typically denoted by n.<br />
    + columns of the data matrix correspond to <strong>variables</strong>, i.e. different characteristics of the units<br /></p>

<p>the <strong>sample distribution</strong> of a variable consists of:<br />
    +  a list of the values of the variable which are observed in the sample<br />
    + the number of times each value occurs (the counts or frequencies of the observed values).<br /></p>

<p>the <strong>frequency table</strong> of all the values and their frequencies is used to show whole sample distribution when the number of different observed values is small.<br />
a <strong>bar chart</strong> is the graphical equivalent of the table of frequencies.<br />
a <strong>histogram</strong> is like a bar chart, but without gaps between bars, and often uses more bars (intervals of values) than is sensible in a table<br /></p>

<p><strong>associations between two variables</strong> :  whether some values of one variable tend to occur frequently together with particular values of another.<br />
    + ‘many’ versus ‘many’: a <strong>scatterplot</strong> shows the values of two measurable variables against each other, plotted as points in a two-dimensional coordinate system.<br />
    + ‘few’ versus ‘many’: side-by-side <strong>boxplots</strong> are useful for comparisons of how the distribution of a measurable variable varies across different groups, i.e. across different levels of a categorical variable.<br />
    + ‘few’ versus ‘few’: two-way <strong>contingency tables</strong> (or <strong>cross-tabulations</strong>) shows the frequencies in the sample of each possible combination of the values of two categorical variables. Such tables often show the percentages within each row or column of the table.<br /></p>

<p><strong>summary (descriptive) statistics</strong> summarise (describe) one feature of the sample distribution in a single number.<br />
<strong>measures of central tendency</strong>:<br />
    + mean (i.e. the average, sample mean or arithmetic mean)<br />
    + median<br />
    + mode.<br /></p>

<p>The <strong>sample mean</strong> (‘arithmetic mean’, ‘mean’ or ‘average’) of a variable <img src="https://latex.codecogs.com/svg.latex?X" /> is denoted <img src="https://latex.codecogs.com/svg.latex?\bar{X}" />, is the ‘sum of the observations’ divided by the ‘number of observations’ (sample size) expressed as <img src="https://latex.codecogs.com/svg.latex?\bar{X}=\frac{\sum_{i=1}^{n}X_i}{n}=\frac{1}{n}\sum_{i=1}^{n}X_i" /><br />
Suppose X has K different values <img src="https://latex.codecogs.com/svg.latex?X_1,X_2,\dots,X_K" /> with corresponding <strong>frequencies</strong> <img src="https://latex.codecogs.com/svg.latex?f_1,f_2,\dots,f_K" />. Therefore, <img src="https://latex.codecogs.com/svg.latex?\sum_{j=1}^{K}f_j=n" /> and:<br />
<img src="https://latex.codecogs.com/svg.latex?\bar{X}=\frac{\sum_{j=1}^{K}f_jX_j}{\sum_{j=1}^{K}f_j}=\frac{f_1X_1+f_2X_2+\dots+f_KX_K}{f_1+f_2+\dots+f_K}=\frac{f_1X_1+f_2X_2+\dots+f_KX_K}{n}" /><br />
    +  as the sum of deviations from the mean is <img src="https://latex.codecogs.com/svg.latex?\sum_{i=1}^{n}(X_i-\bar{X})=0" />, the mean is ‘in the middle’ of the observations <img src="https://latex.codecogs.com/svg.latex?X_1,X_2,\dots,X_n" /> , in the sense that positive and negative values of the <strong>deviations</strong> <img src="https://latex.codecogs.com/svg.latex?X_i-\bar{X}" /> cancel out, when summed over all the observations.<br />
    + the smallest possible value of the sum of squared deviations <img src="https://latex.codecogs.com/svg.latex?\sum_{i=1}^{n}(X_i-C)^2" /> for any constant C is obtained when <img src="https://latex.codecogs.com/svg.latex?C=\bar{X}" /></p>

<p><strong>order statistics</strong> of sample X are values <img src="https://latex.codecogs.com/svg.latex?X_{(1)},X_{(2)},\dots,X_{(n)}" /> ordered from the smallest to the
largest, such that:<br />
    + <img src="https://latex.codecogs.com/svg.latex?X_{(1)}" /> is the smallest observed value (the minimum) of X<br />
    + <img src="https://latex.codecogs.com/svg.latex?X_{(n)}" /> is the largest observed value (the maximum) of X.<br /></p>

<p>the <strong>sample median</strong>, <strong>q<sub>50</sub></strong> of a variable X is the value which is ‘in the middle’ of the ordered sample.<br />
    + if n is odd, then <img src="https://latex.codecogs.com/svg.latex?q_{50}=X_{((n+1)/2)}" />.<br />
    + if n is even, then <img src="https://latex.codecogs.com/svg.latex?q_{50}=(X_{(n/2)}+X_{(n/2+1)})/2" />.<br /></p>

<p>the <strong>(sample) mode</strong> of a variable is the value which has the highest frequency (i.e. appears most often) in the data.<br /></p>

<p>the <strong>sample variance</strong> of a variable X, denoted S<sup>2</sup> (or S<sup>2</sup><sub>X</sub>), defined as <img src="https://latex.codecogs.com/svg.latex?S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2" /> is a measure of the dispersion in a sample dataset.</p>

<p>the <strong>sample standard deviation</strong> of X, denoted S (or S<sub>X</sub>), is the positive square root of the sample variance <img src="https://latex.codecogs.com/svg.latex?S=\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2}" /></p>

<p>the (sample) <strong>quantiles (percentiles)</strong>, q<sub>c</sub> are percentage splits of the sample<br />
    + the <strong>first quartile</strong>, q<sub>25</sub> or Q<sub>1</sub>, is the value which divides the sample into the smallest 25% of observations and the largest 75%.<br />
    + the <strong>third quartile</strong>, q<sub>75</sub> or Q<sub>3</sub>, gives the 75%–25% split<br />
    + the extremes in this spirit are the <strong>minimum</strong>, X<sub>(1)</sub> (the ‘0% quantile’, so to speak), and the <strong>maximum</strong>, X<sub>(n)</sub> (the ‘100% quantile’).<br />
    + <strong>range</strong>: X<sub>(n)</sub> − X<sub>(1)</sub> = maximum − minimum<br />
    + <strong>interquartile range (IQR)</strong>: IQR = q<sub>75</sub> − q<sub>25</sub> = Q<sub>3</sub> - Q<sub>1</sub><br /></p>

<p>a <strong>boxplot</strong> (in full, a box-and-whiskers plot) summarises some key features of a sample distribution using quantiles. The plot is comprised of the following.<br />
    + the line inside the box, which is the median.<br />
    + the box, whose edges are the first and third quartiles (Q<sub>1</sub> and Q<sub>3</sub>). hence the box captures the middle 50% of the data. therefore, the length of the box is the interquartile range.<br />
    + the bottom whisker extends either to the minimum or up to a length of 1.5 times the interquartile range below the first quartile, whichever is closer to the first quartile.<br />
    + the top whisker extends either to the maximum or up to a length of 1.5 times the interquartile range above the third quartile, whichever is closer to the third quartile.<br />
    + points beyond 1.5 times the interquartile range below the first quartile or above the third quartile are regarded as outliers, and plotted as individual points.<br /></p>

<p>the <strong>normal distribution</strong> is by far the most important probability distribution in statistics. This is for three broad reasons.<br />
    + many variables have distributions which are <em>approximately</em> normal, for example heights of humans or animals, and weights of various products.<br />
    + the normal distribution has extremely convenient mathematical properties, which make it a useful default choice of distribution in many contexts.<br />
    + even when a variable is not itself even approximately normally distributed, functions of several observations of the variable (‘sampling distributions’) are often approximately normal, due to the <strong>central limit theorem</strong>. Because of this, the normal distribution has a crucial role in statistical inference.<br />
    + the equation of the normal distribution curve is <img src="https://latex.codecogs.com/svg.latex?f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]" /> for <img src="https://latex.codecogs.com/svg.latex?-\infty&lt;x&lt;\infty" />, where π is the mathematical constant (i.e. π = 3.14159 . . .), and µ and σ<sup>2</sup> are parameters, with −∞ &lt; µ &lt; ∞ and σ<sup>2</sup> &gt; 0.<br />
    + a random variable X with this function is said to have a normal distribution with a mean of µ and a variance of σ<sup>2</sup>, denoted X ∼ N(µ, σ<sup>2</sup>).<br />
    + <strong>the mean µ determines the location of the curve</strong>.<br />
    + <strong>the population variance Var(X) = σ<sup>2</sup> determines the dispersion (spread) of the curve</strong>, which reflects the variance of the whole population, i.e. the variance of a probability distribution.<br /></p>

<p>if a random variable X is normally distributed, then so is <strong>linear transformation Y = aX + b</strong>, where a and b are constants<br />
    + in other words, if X ∼ N(µ, σ<sup>2</sup>) then Y = aX + b ∼ N(aµ+b, a<sup>2</sup>σ<sup>2</sup>)<br />
    + for other families of distributions, the distribution of Y = aX + b is not always in the same family as X.<br />
    + with a = 1/σ and b = -µ/σ we get <img src="https://latex.codecogs.com/svg.latex?Z=\frac{1}{\sigma}X-\frac{\mu}{\sigma}=\frac{X-\mu}{\sigma}\sim N\left(\frac{1}{\sigma}\mu-\frac{\mu}{\sigma},\left(\frac{1}{\sigma}\right)^2\sigma^2\right)=N(0,1)" /><br />
    + the transformed variable Z = (X − µ)/σ is known as a <strong>standardised variable</strong> or a <strong>z-score</strong>.<br />
    + the distribution of the z-score N(0, 1) is known as the <strong>standard normal distribution</strong>.<br /></p>

<p>if we consider the function (X − µ)<sup>2</sup>, i.e. the squared deviation about the population mean, the expectation of this (its probability-weighted average) is <img src="https://latex.codecogs.com/svg.latex?\sigma^2=\text{Var}(X)=\text{E}((X-\mu)^2)=\sum_{i=1}^{n}(x_i-\mu)^2p(x_i)" /><br />
and this represents the <strong>dispersion of a (discrete) probability distribution</strong>.<br /></p>

<p>some probabilities around the mean<br />
    + <img src="https://latex.codecogs.com/svg.latex?P(\mu-\sigma&lt;X&lt;\mu+\sigma)=0.683" />. In other words, about 68.3% of the total probability is within 1 standard deviation of the mean.<br />
    + <img src="https://latex.codecogs.com/svg.latex?P(\mu-1.96\times\sigma&lt;X&lt;\mu+1.96\times\sigma)=0.950" /><br />
    + <img src="https://latex.codecogs.com/svg.latex?P(\mu-2\times\sigma&lt;X&lt;\mu+2\times\sigma)=0.954" /><br />
    + <img src="https://latex.codecogs.com/svg.latex?P(\mu-2.58\times\sigma&lt;X&lt;\mu+2.58\times\sigma)=0.990" /><br />
    + <img src="https://latex.codecogs.com/svg.latex?P(\mu-3\times\sigma&lt;X&lt;\mu+3\times\sigma)=0.997" /><br />
    + <img src="https://latex.codecogs.com/svg.latex?P(-1\leq Z\leq1)\approx0.683" /><br />
    + <img src="https://latex.codecogs.com/svg.latex?P(-2\leq Z\leq2)\approx0.950" /><br />
    + <img src="https://latex.codecogs.com/svg.latex?P(-3\leq Z\leq3)\approx0.997" /><br />
    + only 5% of the time would a standardised value be expected to be beyond ±2 (which we could classify as an <strong>outlier</strong>)<br />
    + only 0.3% of the time beyond ±3 (which we could classify as an <strong>extreme outlier</strong>)<br />
    + values beyond four standard deviations from the mean (i.e. beyond ±4 on a standardised scale) could be considered as <strong>black swan events</strong><br /></p>

<p><b>
Variables used for identification are:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> nominal categorical variables<br />
☐ <code class="highlighter-rouge">WRONG</code> ordinal categorical variables<br />
☐ <code class="highlighter-rouge">WRONG</code> measurable variables<br /></p>

<p><b>
Only proportionate transformations of the form y = bxy=bx are permitted for:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> interval measurable variables<br />
⚡ <code class="highlighter-rouge">CORRECT</code> ratio measurable variables<br /></p>

<p><b>
Statistical inference:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> summarises the data which were collected, in order to make them more understandable<br />
⚡ <code class="highlighter-rouge">CORRECT</code> uses the observed data to draw conclusions about some broader population<br /></p>

<p><b>
A scatterplot is ideal for showing the values of:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> two categorical variables<br />
⚡ <code class="highlighter-rouge">CORRECT</code> two measurable variables<br /></p>

<p><b>
Mean, median and mode are all:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> measures of central tendency<br />
☐ <code class="highlighter-rouge">WRONG</code> measures of spread<br /></p>

<p><b>
For a set of sample data with different observed values:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> <img src="https://latex.codecogs.com/svg.latex?\sum_{i=1}^{n}(X_i-\bar{X})^2&lt;0" /><br />
☐ <code class="highlighter-rouge">WRONG</code> <img src="https://latex.codecogs.com/svg.latex?\sum_{i=1}^{n}(X_i-\bar{X})^2=0" /><br />
⚡ <code class="highlighter-rouge">CORRECT</code> <img src="https://latex.codecogs.com/svg.latex?\sum_{i=1}^{n}(X_i-\bar{X})^2&gt;0" /><br /></p>

<p><b>
For many symmetric distributions, about 2/3 of the observations are:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> between <img src="https://latex.codecogs.com/svg.latex?\bar{X}-S" /> and <img src="https://latex.codecogs.com/svg.latex?\bar{X}+S" />, that is, within one (sample) standard deviation about the (sample) mean<br />
☐ <code class="highlighter-rouge">WRONG</code> between <img src="https://latex.codecogs.com/svg.latex?\bar{X}-2\times S" /> and <img src="https://latex.codecogs.com/svg.latex?\bar{X}+2\times S" />, that is, within two (sample) standard deviations about the (sample) mean<br /></p>

<p><b>
If <img src="https://latex.codecogs.com/svg.latex?X\sim N(\mu,\, \sigma^2)" />, and <img src="https://latex.codecogs.com/svg.latex?Y=aX" />, for a &gt; 0 then:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> <img src="https://latex.codecogs.com/svg.latex?X\sim N(a\mu,\, a\sigma^2)" /><br />
⚡ <code class="highlighter-rouge">CORRECT</code> <img src="https://latex.codecogs.com/svg.latex?X\sim N(a\mu,\, a^2\sigma^2)" /><br /></p>

<p><b>
The variance of a (discrete) random variable, <img src="https://latex.codecogs.com/svg.latex?\sigma^2" />, is a probability-weighted average given by:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> <img src="https://latex.codecogs.com/svg.latex?E(X-\mu)" /><br />
⚡ <code class="highlighter-rouge">CORRECT</code> <img src="https://latex.codecogs.com/svg.latex?E((X-\mu)^2)" /><br /></p>

<p><b>
If <img src="https://latex.codecogs.com/svg.latex?Z \sim N(0,1)" />, then <img src="https://latex.codecogs.com/svg.latex?P(-3\leq Z\leq3)" />:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> ≈ 0.683<br />
☐ <code class="highlighter-rouge">WRONG</code> ≈ 0.950<br />
⚡ <code class="highlighter-rouge">CORRECT</code> ≈ 0.997<br /></p>

<h3 id="on-your-marks-get-set-infer">On Your Marks, Get Set, Infer!</h3>

<p><strong>population</strong> : the aggregate of all the elements, sharing some common set of characteristics, which comprise the universe for the purpose of the problem being investigated.<br />
<strong>census</strong> : a complete enumeration of the elements of a population or study objects.<br />
<strong>sample</strong> : a subgroup of the elements of the population selected for participation in the study.<br /></p>

<p>classification of sampling techniques<br />
    + the <strong>target population</strong> is the collection of elements or objects which possess the information sought by the researcher and about which inferences are to be made<br />
    + there are different types of sampling techniques which can be used in practice, non-probability sampling techniques and probability sampling techniques<br /></p>

<p><strong>non-probability sampling techniques</strong> using when some units in the population do not have a chance of selection in the sample, other individual units in the
population have an unknown probability of being selected, so an inability to measure sampling error.<br />
    + <strong>convenience sampling</strong><br />
    + <strong>judgemental sampling</strong><br />
    + <strong>quota sampling</strong><br />
    + <strong>snowball sampling.</strong><br /></p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>method</td>
      <td>pros</td>
      <td>cons</td>
    </tr>
    <tr>
      <td><strong>convenience</strong></td>
      <td>attempts to obtain a sample of convenient elements</td>
      <td>the cheapest, quickest and most convenient form of sampling</td>
      <td>selection bias and lack of a representative sample</td>
    </tr>
    <tr>
      <td><strong>judgemental</strong></td>
      <td>a form of convenience sampling in which the population elements are selected based on the judgement of the researcher</td>
      <td>low cost, is convenient, not particularly time-consuming and good for ‘exploratory’ research designs</td>
      <td>does not allow generalisations and is subjective due to the judgement of the researcher</td>
    </tr>
    <tr>
      <td><strong>quota</strong></td>
      <td>two-stage restricted judgemental sampling<br />+ developing control categories, or <strong>quota controls</strong>, of population elements<br />+ sample elements are selected based on convenience or judgement</td>
      <td>a sample can be controlled for certain characteristics</td>
      <td>selection bias and there is no guarantee of representativeness of the sample</td>
    </tr>
    <tr>
      <td><strong>snowball</strong></td>
      <td>+ an initial group of respondents is selected, usually at random<br />+ after being interviewed, these respondents are asked to identify others who belong to the target population of interest<br />+ subsequent respondents are selected based on these referrals</td>
      <td>able to increase the chance of locating the desired characteristic in the population and is also fairly cheap</td>
      <td>time-consuming</td>
    </tr>
  </tbody>
</table>

<p><strong>sample surveys</strong> are how new data are collected on a population and tend to be based on samples rather than a census.</p>

<p>types of error<br />
    • <strong>sampling error</strong> occurs as a result of us selecting a sample, rather than performing a census (where a total enumeration of the population is undertaken).<br />
         + It is attributable to random variation due to the sampling scheme used.<br />
         + For probability sampling, we can estimate the statistical properties of the sampling error, i.e. we can compute (estimated) standard errors which facilitate the use of hypothesis testing and the construction of confidence intervals.<br />
    • <strong>non-sampling error</strong> is a result of (inevitable) failures of the sampling scheme.<br />
         + <strong>selection bias</strong> – this may be due to i. the sampling frame not being equal to the target population, or ii. in cases where the sampling scheme is not strictly adhered to, or iii. non-response bias.<br />
         + <strong>response bias</strong> – the actual measurements might be wrong, for example ambiguous question wording, misunderstanding of a word in a questionnaire, or sensitivity of information which is sought. Interviewer bias is another aspect of this.<br /></p>

<p>a <strong>pilot survey</strong> can control or allow both kinds of error for more effectively.<br />
    • to find the standard error which can be attached to different kinds of questions and hence to underpin the sampling design chosen<br />
    • to sort out non-sampling questions, such as:<br />
         + do people understand the questionnaires?<br />
         + are our interviewers working well?<br />
         + are there particular organisational problems associated with this enquiry?<br /></p>

<p><strong>probability sampling techniques</strong> mean every population element has a known, non-zero probability of being selected in the sample.<br />
    + possible to estimate the margins of sampling error, therefore all statistical techniques (such as confidence intervals and hypothesis testing) can be applied.<br /></p>

<p>a <strong>sampling frame</strong> is a list of all population elements.<br />
    + adequate (does it represent the target population?)<br />
    + complete (are there any missing units, or duplications?)<br />
    + accurate (are we researching dynamic populations?)<br />
    + convenient (is the sampling frame readily accessible?).<br /></p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>method</td>
      <td>pros</td>
      <td>cons</td>
    </tr>
    <tr>
      <td><strong>simple random</strong></td>
      <td>every element is selected independently of every other element<br />+ each element in the population has a known and equal probability of selection.<br />+ each possible sample of a given size, n, has a known and equal probability of being the sample which is actually selected.</td>
      <td>simple to understand and results are readily projectable</td>
      <td>may be difficulty constructing the sampling frame, lower precision (relative to other probability sampling methods) and there is no guarantee of sample representativeness</td>
    </tr>
    <tr>
      <td><strong>systematic</strong></td>
      <td>selecting a random starting point and then picking every <em>i</em>th element in succession from the sampling frame<br />+ the <strong>sampling interval</strong>, i, is determined by dividing the population size, N, by the sample size, n, and rounding to the nearest integer<br />+ if the <strong>ordering of the elements</strong> is related to the characteristic of interest, it <em>increases the representativeness of the sample</em><br />+ if the ordering of the elements produces a <strong>cyclical pattern</strong>, it may actually <em>decrease the representativeness of the sample</em></td>
      <td>+ may or may not increase representativeness – it depends on whether there is any ‘ordering’ in the sampling frame<br />+ easier to implement relative to SRS</td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>stratified</strong></td>
      <td>+ a two-step process in which the population is partitioned (divided up) into subpopulations known as <strong>strata</strong><br />+ elements are selected from each stratum by a random procedure, usually SRS<br />+ a major objective of stratified sampling is to increase the precision of statistical inference without increasing cost<br />• the elements <em>within a stratum</em> should be as <em>homogeneous</em> as possible (i.e. as similar as possible), but the elements <em>between strata</em> should be as <em>heterogeneous</em> as possible (i.e. as different as possible)</td>
      <td>includes all important subpopulations and ensures a high level of precision</td>
      <td>it might be difficult to select relevant stratification factors and the stratification process itself might not be feasible in practice if it was not known to which stratum each population element belonged</td>
    </tr>
    <tr>
      <td><strong>cluster</strong></td>
      <td>+ the target population is first divided into mutually exclusive and collectively exhaustive subpopulations known as <strong>clusters</strong><br />+ a random sample of clusters is then selected, based on a probability sampling technique such as SRS<br />• elements <em>within</em> a cluster should be as <em>heterogeneous</em> as possible, but clusters themselves should be as <em>homogeneous</em> as possible</td>
      <td>easy to implement and cost effective</td>
      <td>suffers from a lack of precision and it can be difficult to compute and interpret results</td>
    </tr>
    <tr>
      <td><strong>multistage</strong></td>
      <td>selection is performed at two or more successive stages<br />+ at the first stage, large ‘compound’ units are sampled (<strong>primary units</strong>)<br />+ then several sampling stages of this type may be performed until we at last sample the basic units.</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>A <strong>simple random sample</strong> is a sample selected by a process where every possible sample (of the same size, n) has the same probability of selection.<br />
The selection process is left to chance, therefore eliminating the effect of <strong>selection bias</strong>.<br />
<strong>Parameters</strong> of a population are particular characteristics of interest such as the mean µ, and variance σ<sup>2</sup>. If we do not have population
data, the parameter values will be unknown.<br /></p>

<p><strong>Statistical inference</strong> is the process of estimating the (unknown) parameter values using the (known) sample data.<br />
We use a statistic (called an <strong>estimator</strong>) calculated from sample observations to provide a <strong>point estimate</strong> of a parameter.<br />
The maximum possible <strong>absolute deviations</strong> of the sample mean from the population mean is the max distance <img src="https://latex.codecogs.com/svg.latex?\left|\bar{x}-\mu\right|" /> where µ is the population mean <img src="https://latex.codecogs.com/svg.latex?\mu=\frac{1}{N}\sum_{i=1}^{N}X_i" /> and <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}" /> are values of sample mean (estimator) <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i" />.<br />
The <strong>frequency distribution</strong> is the frequency of each possible value of <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}" />, known as the <strong>sampling distribution</strong> of <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}" />.<br />
    +  The sampling distribution is a central and vital concept in statistics. <br />
    +  It can be used to evaluate how ‘good’ an estimator is.<br />
    + Specifically, we care about how ‘close’ the estimator is to the population parameter of interest.<br /></p>

<p>Different samples yield different sample mean values<br />
    → estimators (of which <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}" /> is an example) are random variables<br />
    → <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}" /> is our estimator of µ, and the observed value of <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}" />, denoted <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}" />, is a point estimate.<br /></p>

<p>The mean of a sampling distribution is called the <strong>expected value</strong> of the estimator, denoted by E(·).<br />
Hence the expected value of the sample mean is <img src="https://latex.codecogs.com/svg.latex?\text{E}(\large\bar{X})" />.<br />
An <strong>unbiased estimator</strong> has its expected value equal to the parameter being estimated, ie. <img src="https://latex.codecogs.com/svg.latex?\text{E}(\bar{X})=\mu" />.<br /></p>

<p>The <strong>sampling variance</strong> is the mean of the squared deviations about the mean of the sampling distribution.<br />
    + a larger  population variance σ<sup>2</sup> should lead to a larger sampling variance. For population size N and sample size n, we note the following result when sampling without replacement <img src="https://latex.codecogs.com/svg.latex?\text{Var}(\large\bar{X})=\frac{N-n}{N-1}\times\frac{\sigma^2}{n}" />.<br />
    + the <strong>standard error</strong> is the standard deviation of the sampling distribution, <img src="https://latex.codecogs.com/svg.latex?\text{S.E.}(\large\bar{X})=\sqrt{\text{Var}(\large\bar{X})}=\sqrt{\frac{N-n}{N-1}\times\frac{\sigma^2}{n}}=\sigma_{\bar{X}}" />.<br />
    + as the sample size n increases, the sampling variance decreases, i.e. the <strong>precision</strong> increases.<br />
    + provided the <strong>sampling fraction</strong> n/N  is small, the term <img src="https://latex.codecogs.com/svg.latex?\frac{N-n}{N-1}\approx1" />, so the precision depends effectively on n only. Hence, <img src="https://latex.codecogs.com/svg.latex?\text{Var}(\large\bar{X})=\frac{N-n}{N-1}\times\frac{\sigma^2}{n}\approx\frac{\sigma^2}{n}=\frac{\text{Var}(X)}{n}" /><br /></p>

<p>When the distribution of X is normal, the sampling distribution of <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}" /> is also normal. Suppose that <img src="https://latex.codecogs.com/svg.latex?\{X_1,X_2,\dots,X_n\}" /> is a random sample from a normal distribution with mean <img src="https://latex.codecogs.com/svg.latex?\mu" /> and variance σ<sup>2</sup>. Therefore,<br />
<img src="https://latex.codecogs.com/svg.latex?\large\bar{X}\sim N\left(\mu,\frac{\sigma^2}{n}\right)" /><br />
We note <img src="https://latex.codecogs.com/svg.latex?\text{E}(\large\bar{X})=\text{E}(X)=\mu" /> and <img src="https://latex.codecogs.com/svg.latex?\text{Var}(\large\bar{X})=\text{Var}(X)/n=\sigma^2/n" />. The standard error is σ/√n.<br />
    + In an individual sample, <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}" /> is not usually equal to <img src="https://latex.codecogs.com/svg.latex?\mu" />, the expected value of the population.<br />
    + However, over repeated samples the values of <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}" /> are centred at <img src="https://latex.codecogs.com/svg.latex?\mu" /><br /></p>

<p><strong>Confidence intervals</strong> communicate the level of imprecision by converting a point estimate into an interval estimate.<br />
    + an x% confidence interval <strong>covers</strong> the unknown parameter with x% probability <strong>over repeated samples</strong><br />
    + the shorter the confidence interval, the more reliable the estimate.<br /></p>

<p>If we assume we have either i. <strong>known σ</strong>, or ii. <strong>unknown σ but a large sample size</strong>, say n ≥ 50, then the formulae for the <strong>endpoints</strong> of a confidence interval for a single mean are: i. <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}\pm z\times\frac{\sigma}{\sqrt{n}}" /> and ii. <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}\pm z\times\frac{s}{\sqrt{n}}" /><br />
Here <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}" /> is the sample mean, σ is the population standard deviation, s is the sample standard deviation, n is the sample size and z is the <strong>confidence coefficient</strong>, reflecting the confidence level.<br /></p>

<p>The confidence interval for a mean is: <strong>best guess ± margin of error</strong><br />
where <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}" /> is the best guess, and the margin of error is: i. <img src="https://latex.codecogs.com/svg.latex?z\times\frac{\sigma}{\sqrt{n}}" /> and ii. <img src="https://latex.codecogs.com/svg.latex?z\times\frac{s}{\sqrt{n}}" />.<br /></p>

<p>There are <strong>three influences on the size of the margin of error</strong> (and hence on the width of the confidence interval):<br />
• as n ↑ ⇒ margin of error ↓ ⇒ width ↓<br />
• as σ ↑ ⇒ margin of error ↑ ⇒ width ↑<br />
• as confidence level ↑ ⇒ margin of error ↑ ⇒ width ↑<br /></p>

<p><b>
Non-probability sampling techniques, such as convenience and quota sampling, suffer from selection bias.<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> True<br />
☐ <code class="highlighter-rouge">WRONG</code> False<br /></p>

<p><b>
Response bias is an example of:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> sampling error<br />
⚡ <code class="highlighter-rouge">CORRECT</code> non-sampling error<br /></p>

<p><b>
In probability sampling:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> there is a known, non-zero probability of a population element being selected<br />
☐ <code class="highlighter-rouge">WRONG</code> some population elements have a zero probability of being selected, while others have an unknown probability of being selected<br /></p>

<p><b>
In systematic sampling, N/n, is referred to as:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> the sampling interval<br />
☐ <code class="highlighter-rouge">WRONG</code> the sampling fraction<br /></p>

<p><b>
In stratified random sampling, elements between strata should be:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> as heterogeneous as possible<br />
☐ <code class="highlighter-rouge">WRONG</code> as homogeneous as possible<br /></p>

<p><b>
A sampling distribution is the:<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> probability (frequency) distribution of a parameter<br />
⚡ <code class="highlighter-rouge">CORRECT</code> probability (frequency) distribution of a statistic<br /></p>

<p><b>
The mean of a sampling distribution is called:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> the expected value of the estimator<br />
☐ <code class="highlighter-rouge">WRONG</code> the variance of the estimator<br /></p>

<p><b>
An x% confidence interval covers the unknown parameter<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> with x% probability over repeated samples<br />
☐ <code class="highlighter-rouge">WRONG</code> with a probability of x%<br /></p>

<p><b>
Which of the following does not influence the size of the margin of error when considering the confidence interval for a population mean?<br />
</b>
☐ <code class="highlighter-rouge">WRONG</code> the sample size, n<br />
⚡ <code class="highlighter-rouge">CORRECT</code> the sample mean, <img src="https://latex.codecogs.com/svg.latex?\large\bar{x}" /><br />
☐ <code class="highlighter-rouge">WRONG</code> the standard deviation, σ or s<br /></p>

<p><b>
Other things equal, as the sample size nn increases the margin of error:<br />
</b>
⚡ <code class="highlighter-rouge">CORRECT</code> decreases<br />
☐ <code class="highlighter-rouge">WRONG</code> stays the same<br />
☐ <code class="highlighter-rouge">WRONG</code> increases<br /></p>

<h3 id="to-p-or-not-to-p">To p Or Not To p?</h3>

<p><strong>hypothesis testing</strong> : decision theory whereby we make a binary decision between two competing hypotheses:<br />
    • <strong>H0 = the null hypothesis</strong><br />
    • <strong>H1 = the alternative hypothesis.</strong><br />
The binary decision is whether to ‘<strong>reject H0</strong>’ or ‘<strong>fail to reject H0</strong>’<br /></p>

<p>In any hypothesis test there are two types of <strong>inferential decision error</strong> which could be committed.<br />
    • <strong>Type I error</strong>: rejecting H0 when it is true. This can be thought of as a ‘<strong>false positive</strong>’. Denote the probability of this type of error by <strong>α</strong>.<br />
    • <strong>Type II error</strong>: failing to reject H0 when it is false. This can be thought of as a ‘<strong>false negative</strong>’. Denote the probability of this type of error by <strong>β</strong>.<br /></p>

<p>The <strong>decision space</strong></p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>H0 not rejected</td>
      <td>H0 rejected</td>
    </tr>
    <tr>
      <td>H0 true</td>
      <td>Correct decision</td>
      <td>Type I error</td>
    </tr>
    <tr>
      <td>H1 true</td>
      <td>Type II error</td>
      <td>Correct decision</td>
    </tr>
  </tbody>
</table>

<p>The complement of a Type II error, that is 1 − β, is called the <strong>power</strong> of the test – the probability that the test will reject a false null hypothesis.</p>

<p>The <strong>conditional probabilities</strong></p>

<table>
  <tbody>
    <tr>
      <td> </td>
      <td>H0 not rejected</td>
      <td>H0 rejected</td>
    </tr>
    <tr>
      <td>H0 true</td>
      <td>1 − α</td>
      <td>P(Type I error) = α</td>
    </tr>
    <tr>
      <td>H1 true</td>
      <td>P(Type II error) = β</td>
      <td>Power = 1 − β</td>
    </tr>
  </tbody>
</table>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>P(H0 not rejected | H0 is true) = 1 − α
    P(H0 rejected | H0 is true) = α
P(H0 not rejected | H1 is true) = β
    P(H0 rejected | H1 is true) = 1 − β
</code></pre></div></div>

<p>The <strong>significance level as the complement of the confidence level</strong>:<br />
    • a 90% confidence level equates to a 10% significance level<br />
    • a 95% confidence level equates to a 5% significance level<br />
    • a 99% confidence level equates to a 1% significance level<br /></p>

<p>A <strong>p-value</strong> is the probability of the event that the ‘test statistic’ takes the observed value or more extreme (i.e. more unlikely) values under H0. It is a measure of the discrepancy between the hypothesis H0 and the data evidence.<br />
• A ‘small’ p-value indicates that H0 is not supported by the data.<br />
• A ‘large’ p-value indicates that H0 is not inconsistent with the data.<br />
So p-values may be seen as a <strong>risk measure</strong> of rejecting H0.<br /></p>

<p>The magnitude of the p-value (compared with α) determines whether or not H0 is rejected.<br />
Therefore, it is important to consider <strong>two key influences</strong> on the magnitude of the p-value: the effect size and the sample size.<br /></p>

<p>The <strong>effect size</strong> reflects the difference between what you would expect to observe if the null hypothesis is true and what is actually observed in a random experiment. Hence <strong>as the effect size gets larger, the p-value gets smaller</strong> (and so is more likely to be below α).<br />
It’s called the <strong>inverse relationship between the effect size and the p-value</strong>.<br />
<strong>sensitivity analysis</strong> : the pure influence of the effect size on the p-value while controlling for (fixing) the sample size.<br /></p>

<p>Sample size influence : <strong>a larger sample size should lead to a more representative random sample</strong> and the characteristics of the sample should more closely resemble those of the population distribution from which the sample is drawn.<br />
For a non-zero effect size, <strong>the p-value decreases as the sample size increases</strong>.<br />
It’s called the <strong>inverse relationship between the sample size and the p-value</strong>.<br /></p>

<p>The <strong>central limit theorem (CLT)</strong> : the normal sampling distribution of <img src="https://latex.codecogs.com/svg.latex?\bar{X}" /> which holds exactly for random samples from a normal distribution, also holds approximately for random samples from nearly any distribution.<br />
<img src="https://latex.codecogs.com/svg.latex?\lim_{n\to\infty}P\left(\frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}}\leq z\right)=\Phi(z)" /> for any z<br />
where,<br />
<img src="https://latex.codecogs.com/svg.latex?\{X_1,X_2,\dots,X_n\}" /> is a random sample from a population distribution which mean <img src="https://latex.codecogs.com/svg.latex?\text{E}(X_i)=\mu&lt;\infty" /> and variance <img src="https://latex.codecogs.com/svg.latex?\text{Var}(X_i)=\sigma^2&lt;\infty" /><br />
<img src="https://latex.codecogs.com/svg.latex?\Phi(z)=P(Z\leq z)" /> denotes a cumulative probability of the standard normal distribution.<br /></p>

<p>The <img src="https://latex.codecogs.com/svg.latex?\lim_{n\to\infty}" /> indicates that this is an <strong>asymptotic</strong> result, i.e. one which holds increasingly well
as n increases, and exactly when the sample size is infinite.</p>

<p>When n is sufficiently large, then <img src="https://latex.codecogs.com/svg.latex?\bar{X}\sim N\left(\mu,\frac{\sigma^2}{n}\right)" /> approximately.<br />
We can then say that <img src="https://latex.codecogs.com/svg.latex?\bar{X}" /> is <strong>asymptotically normally distributed</strong> with mean µ and variance σ<sup>2</sup>/n.<br />
<strong>For many distributions, n &gt; 50 is sufficient for the approximation to be reasonably accurate.</strong><br /></p>

<p>Assume n is sufficiently large and X ∼ Bernoulli(π). Then <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}" /> is the <strong>sample proportion</strong> of the value X = 1 because <img src="https://latex.codecogs.com/svg.latex?\large\bar{X}=\sum_{i-1}^{n}X_i/n=m/n" />, where m is the number of observations for which <img src="https://latex.codecogs.com/svg.latex?X_i=1" />.<br />
    + since from CLT <img src="https://latex.codecogs.com/svg.latex?\bar{X}\sim N\left(\mu,\frac{\sigma^2}{n}\right)" /> then <img src="https://latex.codecogs.com/svg.latex?\text{E}(X)=0\times(1-\pi)+1\times\pi=\pi=\mu" /> and <img src="https://latex.codecogs.com/svg.latex?\text{Var}(X)=\pi(1-\pi)=\sigma^2" /><br />
    + we have <img src="https://latex.codecogs.com/svg.latex?\bar{X}=P\to N\left(\mu,\frac{\sigma^2}{n}\right)=N\left(\pi,\frac{\pi(1-\pi)}{n}\right)" /> as <img src="https://latex.codecogs.com/svg.latex?n\to\infty" /><br />
    + hence the sample proportion is equal to the population proportion, on <em>average</em> <img src="https://latex.codecogs.com/svg.latex?\text{E}(\large\bar{X})=\text{E}(P)=\pi" /><br />
    + and the sampling variance tends to zero as the sample size tends to infinity <img src="https://latex.codecogs.com/svg.latex?\text{Var}(P)\to0\text{ as }n\to\infty" /><br /></p>

<p>Confidence intervals<br />
    + confidence interval for a mean <strong>best guess ± margin of error</strong><br />
    + if p is the observed sample proportion then <strong>point estimate = p</strong><br />
    + <strong>margin of error = confidence coefficient × standard error</strong><br />
    + the (estimated) standard error is <img src="https://latex.codecogs.com/svg.latex?\sqrt{\frac{p(1-p)}{n}}" /><br />
    + Therefore, a <strong>confidence interval for a proportion</strong> is given by <img src="https://latex.codecogs.com/svg.latex?p\pm z\times\sqrt{\frac{p(1-p)}{n}}" /><br /></p>

<h3 id="applications">Applications</h3>

<p><strong>Decision making under uncertainty</strong> are decisions are taken in the present with <strong>uncertain future outcomes</strong>.<br />
<strong>Decision tree analysis</strong> is an interesting modelling technique which allows us to incorporate probabilities in the decision-making process to model and quantify the uncertainty.<br /></p>

<p>A standard decision tree consists of the following components:<br />
• <strong>Decision nodes</strong> indicate that the decision-maker has to make a choice, denoted ◻.<br />
• <strong>Chance nodes</strong> indicate the resolution of uncertainty, denoted ○.<br />
• <strong>Branches</strong> represent the choices available to the decision-maker (if leading from decision nodes) or the possible outcomes if uncertainty is resolved (leading from chance nodes).<br />
• <strong>Probabilities</strong> are written at the branches leading from chance nodes.<br />
• <strong>Payoffs</strong> are written at the end of the final branches.<br /></p>

<p>In order to solve the decision tree we calculate the <strong>expected monetary value (EMV)</strong> of each option (advertise and not advertise) and proceed whereby the decision-maker maximises <strong>expected profits</strong>.</p>

<p>Degrees of risk aversion<br />
    + A decision-maker is <strong>risk-averse</strong> if s/he prefers the certain outcome of £x over a risky project with a mean (EMV) of £x.<br />
    + A decision-maker is <strong>risk-loving</strong> (also known as risk-seeking) if s/he prefers a risky project with a mean (EMV) of £x over the certain outcome of £x.<br />
    + A decision-maker is <strong>risk-neutral</strong> if s/he is indifferent between a sure payoff and an uncertain outcome with the same expected monetary value.<br />
    + The <strong>certainty equivalent (CE)</strong> of a risky project is the amount of money which makes the decision-maker indifferent between receiving this amount for sure and the risky project.<br /></p>

<p>The <strong>risk premium</strong> (of a risky project) is defined as: <strong>EMV − CE</strong>.<br />
    + CE &lt; EMV ⇒ risk-averse<br />
    + CE = EMV ⇒ risk-neutral<br />
    + CE &gt; EMV ⇒ risk-loving<br /></p>

<p><strong>Linear regression analysis</strong> aims to model an explicit relationship between one <strong>dependent variable</strong>, denoted as y, and one or more <strong>regressors</strong> (also called covariates, or independent variables), denoted as x<sub>1</sub>, . . . , x<sub>p</sub>.<br />
    + to <strong>understand</strong> how y depends on x<sub>1</sub>, . . . , x<sub>p</sub><br />
    + to <strong>predict</strong> or control the unobserved y based on the observed x<sub>1</sub>, . . . , x<sub>p</sub><br /></p>

<p><strong>Linear programming</strong> is probably one of the most-used type of <strong>quantitative business model</strong>. It can be applied in any environment where <strong>finite resources</strong> must be allocated to competing activities or processes for maximum benefit.</p>

<p>All <strong>optimisation models</strong> have several common elements.<br />
• <strong>Decision variables</strong>, or the variables whose values the decision-maker is allowed to choose. These are the variables which a company must know to function properly – they determine everything else.<br />
• <strong>Objective function</strong> to be optimised – either maximised or minimised.<br />
• <strong>Constraints</strong> which must be satisfied – physical, logical or economic restrictions, depending on the nature of the problem.<br /></p>

<p>Solving optimisation problems<br />
• first is the <strong>model development step</strong><br />
    + the decision variables, the objective and the constraints<br />
    + how everything fits together, i.e. develop correct algebraic expressions and relate all variables with appropriate formulae.<br />
• second is to <strong>optimise</strong><br />
    + A feasible solution is a solution which satisfies all of the constraints.<br />
    + The feasible region is the set of all feasible solutions.<br />
    + An infeasible solution violates at least one of the constraints.<br />
    + The optimal solution is the feasible solution which optimises the objective.<br />
• third is to perform a <strong>sensitivity analysis</strong><br />
    + to what extent is the final solution sensitive to parameter values used in the model<br /></p>


        </div>
    </div>
</body>
</html>